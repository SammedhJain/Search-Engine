{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFboIGSXdfyk",
        "outputId": "1a03b5dc-191e-48ac-a357-8612489e066f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.8/dist-packages (2.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAHMXAiUv054"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "from autocorrect import Speller"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGOGj4LlCqVm",
        "outputId": "d0f78a44-ef1e-4314-9880-01ba7bafb37b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YduoVHwticEi"
      },
      "source": [
        "f=open('drive/My Drive/AIR/inverted_index1.json','r')\n",
        "inverted_index=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/record_list.json','r')\n",
        "record_list=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/permuterm1.json','r')\n",
        "permuterm_index=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/snippet_df.txt','r')\n",
        "snippet_df=eval(f.read())\n",
        "doc_location='drive/My Drive/archive/TelevisionNews/'"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2z4C6f5jlvD",
        "outputId": "4f5a9f41-a089-44d1-91c6-a28540af56f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words(\"english\")\n",
        "stopWords.append(\"i\\'ve\")\n",
        "stopWords.append(\"i\\'m\")\n",
        "stopWords.append(\"he\\'d\")\n",
        "stopWords.append(\"she\\'d\")\n",
        "stopWords.append(\"i\\'d\")\n",
        "stopWords.append(\"i\\'ll\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word in sentence.split():\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "def special_characters_removal(word):\n",
        "  special=list(string.punctuation)\n",
        "  special.append(' ')\n",
        "  special.remove('*')\n",
        "  x=[]\n",
        "  for i in word:\n",
        "    if i not in special:\n",
        "      x.append(i)\n",
        "    else:\n",
        "      x.append(' ')\n",
        "  return ''.join(x) \n",
        "\n",
        "def preprocessing(snippet):\n",
        "  snippet=snippet.split()\n",
        "  res=[]\n",
        "  for i in range(len(snippet)):\n",
        "    if snippet[i] not in stopWords and len(snippet[i])>1:\n",
        "      res.extend(special_characters_removal(snippet[i]).split())\n",
        "  snippet=\" \".join(res)\n",
        "  snippet=lemmatize_sentence(snippet)\n",
        "  return snippet.lower()  "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7ainrEvkU60"
      },
      "source": [
        "## **`Boolean Queries`**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY9cQLV9kRh0"
      },
      "source": [
        "def mergeand(left_operand,right_operand):\n",
        "  doc_list=set(left_operand.keys()).intersection(set(right_operand.keys()))\n",
        "  final_record_list={}\n",
        "  for k in doc_list:\n",
        "    x=left_operand[k].intersection(right_operand[k])\n",
        "    if x:\n",
        "      final_record_list[k]=x\n",
        "  return final_record_list\n",
        "\n",
        "def mergeor(left_operand,right_operand):\n",
        "  doc_list=set(left_operand.keys()).union(set(right_operand.keys()))\n",
        "  final_record_list={k:left_operand[k].union(right_operand[k] if k in right_operand else set()) if k in left_operand else right_operand[k] for k in doc_list}\n",
        "  return final_record_list\n",
        "\n",
        "def get_not_posting_list(operand):\n",
        "  files=set(record_list.keys())\n",
        "  post_list={k:set(map(str,range(record_list[k][0]))).difference(operand[k] if k in operand else {}) for k in files}\n",
        "  return post_list\n",
        "\n",
        "def show_records(final):\n",
        "  result=[]\n",
        "  for i in final:\n",
        "    doc_l=doc_location+i\n",
        "    df=pd.read_csv(doc_l)\n",
        "    for j in final[i]:\n",
        "      result.append(df.loc[int(j)])\n",
        "  return result\n",
        "\n",
        "def get_posting_list(term):\n",
        "  if term not in inverted_index:\n",
        "    return dict()\n",
        "  x=list(inverted_index[term].keys())\n",
        "  x.remove('doc_frequency')\n",
        "  post_list={k:set(inverted_index[term][k]['records'].keys()) for k in x}\n",
        "  return post_list\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDGsUDH29pFZ"
      },
      "source": [
        "def parsebooleanquery(query):\n",
        "  query=query.replace('(',' ( ')\n",
        "  query=query.replace(')',' ) ')\n",
        "  precedence={}\n",
        "  precedence['NOT']=3\n",
        "  precedence['OR']=2\n",
        "  precedence['AND']=1\n",
        "  precedence['(']=0\n",
        "  precedence[')']=0\n",
        "  operand_stack=[]\n",
        "  operator_stack=[]\n",
        "  postfix_stack=[]\n",
        "  j=0\n",
        "  l=query.split()\n",
        "  while(j<len(l)):\n",
        "    i=l[j]\n",
        "    if i==\"(\":\n",
        "      operator_stack.append(i)\n",
        "    elif i==\")\":\n",
        "      while( operator_stack and operator_stack[-1]!=\"(\" ):\n",
        "        postfix_stack.append(operator_stack.pop())\n",
        "      operator_stack.pop()\n",
        "    elif i in precedence:\n",
        "      while(operator_stack and precedence[operator_stack[-1]]>precedence[i]):\n",
        "        postfix_stack.append(operator_stack.pop())\n",
        "      operator_stack.append(i)\n",
        "    else:\n",
        "      x=i\n",
        "      j+=1\n",
        "      while(j<len(l) and l[j] not in precedence):\n",
        "        x=x+\" \"+l[j]\n",
        "        j+=1\n",
        "      if not operator_stack or operator_stack[-1]!=\"NOT\":\n",
        "        operand_stack.append(x)\n",
        "      postfix_stack.append(x)\n",
        "      j-=1\n",
        "    j+=1\n",
        "  while(operator_stack):\n",
        "    postfix_stack.append(operator_stack.pop())\n",
        "  return postfix_stack,operand_stack\n",
        "\n",
        "def eval_query(postfix_stack):\n",
        "  result=[]\n",
        "  for i in postfix_stack:\n",
        "    if i==\"AND\":\n",
        "      left_operand=result.pop()\n",
        "      right_operand=result.pop()\n",
        "      result.append(mergeand(left_operand,right_operand))\n",
        "    elif i==\"OR\":\n",
        "      left_operand=result.pop()\n",
        "      right_operand=result.pop()\n",
        "      result.append(mergeor(left_operand,right_operand))\n",
        "    elif i==\"NOT\":\n",
        "      operand=result.pop()\n",
        "      result.append(get_not_posting_list(operand))\n",
        "    else:\n",
        "      result.append(parsephrasequery(i) if '*' not in i else eval_wildcard_query(i))\n",
        "  return result[0]\n",
        "\n",
        "def get_results(query):\n",
        "  p_stack,o_stack=parsebooleanquery(query)\n",
        "  return eval_query(p_stack),o_stack"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqdon_QJeH_q"
      },
      "source": [
        "# Phrase query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhFn1RAWixj4"
      },
      "source": [
        "def parsephrasequery(query):\n",
        "  query=preprocessing(spellcheck(query))\n",
        "  tokens=query.split()\n",
        "  intermediate_result=get_posting_list(tokens[0])\n",
        "  for i in range(1,len(tokens)):\n",
        "    intermediate_result=mergeand(intermediate_result,get_posting_list(tokens[i]))\n",
        "  final_record_list=mergephrase(tokens,intermediate_result) if len(tokens)>1 else intermediate_result\n",
        "  return final_record_list\n",
        "\n",
        "def get_positional_list(doc_name, record_no, term):\n",
        "  return inverted_index[term][doc_name]['records'][record_no]['position'] \n",
        "\n",
        "def mergephrase(tokens,intersect_list):\n",
        "  final_record_list={}\n",
        "  for doc in intersect_list:\n",
        "    records=intersect_list[doc]\n",
        "    for record in records:\n",
        "      temp_pos=[]\n",
        "      for i in range(len(tokens)):\n",
        "        temp_pos.append(set(map(lambda x:x-i,get_positional_list(doc, record, tokens[i]))))\n",
        "      if (set(temp_pos[0].intersection(*temp_pos))):\n",
        "        if (doc not in final_record_list):\n",
        "          final_record_list[doc]=set()\n",
        "        final_record_list[doc].add(record)\n",
        "  return final_record_list  "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJwLRUHlPe4T"
      },
      "source": [
        "# Wildcard Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr6lzEgAPmBa"
      },
      "source": [
        "def splitquery(query):\n",
        "  query1=query.split(\"*\")\n",
        "  if query1[0]=='' and len(query1)==3 and query1[2]=='':\n",
        "    return [query]\n",
        "  elif len(query1)==2 and (query1[0]=='' or  query1[1]==''):\n",
        "    return [query]\n",
        "  else:\n",
        "    result=[]\n",
        "    if query1[0]=='':\n",
        "      result.append(\"*\"+query1[1]+\"*\")\n",
        "      i=2\n",
        "    else:\n",
        "      result.append(query1[0]+\"*\")\n",
        "      i=1\n",
        "    while(i<len(query1)-1):\n",
        "      result.append(\"*\"+query1[i]+\"*\")\n",
        "      i+=1\n",
        "    if query1[i]!='':\n",
        "      result.append(\"*\"+query1[i])\n",
        "    return result\n",
        "  \n",
        "def permuterm_form(query):\n",
        "  query=query.split('*')\n",
        "  if query[0]=='' and len(query)==3 and query[2]=='': #*X*\n",
        "    actual_query=query[1]\n",
        "  elif query[0]=='' and len(query)==2:  #*X\n",
        "    actual_query=query[1]+\"$\"\n",
        "  elif query[1]=='' and len(query)==2:  #X*\n",
        "    actual_query=\"$\"+query[0]\n",
        "  return actual_query\n",
        "\n",
        "def parsewildcardquery(input_query):\n",
        "  x=splitquery(input_query)\n",
        "  result=get_wildcard_result(permuterm_form(x[0]))\n",
        "  for query in x[1:]:\n",
        "    result=result.intersection(get_wildcard_result(permuterm_form(query)))\n",
        "  tokens=input_query.split('*')\n",
        "  result1=[]\n",
        "  for i in result:\n",
        "    y=i\n",
        "    flag=0\n",
        "    for j in tokens:\n",
        "      if j=='':\n",
        "        pass\n",
        "      else:\n",
        "        pos=y.find(j)\n",
        "        if pos==-1:\n",
        "          flag=1\n",
        "          break\n",
        "        else:\n",
        "          y=y[pos+len(j):]\n",
        "    if flag==0:\n",
        "      result1.append(i)\n",
        "  return result1\n",
        "\n",
        "def get_wildcard_result(query):\n",
        "  result=[]\n",
        "  for i in permuterm_index:\n",
        "    if i.startswith(query):\n",
        "      result.append(permuterm_index[i])\n",
        "  return set(result)\n",
        "\n",
        "def eval_wildcard_query(query):\n",
        "  x=parsewildcardquery(query)\n",
        "  result=get_posting_list(x[0])\n",
        "  for i in x[1:]:\n",
        "    result=mergeor(get_posting_list(i),result)\n",
        "  return result"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW8bQaUgNCLt"
      },
      "source": [
        "# Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukNO6WWRQcup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a6d91f-5239-46c4-ae63-08f6d3c2092f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "preprocessed_snippet_df=list(map(lambda x:preprocessing(x[0]),snippet_df))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6h69gzmX-ui"
      },
      "source": [
        "def ranked_results(query,k):\n",
        "  f=open('text2.txt','w')\n",
        "  t1=time.time()\n",
        "  results,tokens=get_results(query)\n",
        "  if not results:\n",
        "    return \"No results obtained.\"\n",
        "  tokens1=[]\n",
        "  for i in range(len(tokens)):\n",
        "    if '*' in tokens[i]:\n",
        "      tokens1.extend(list(parsewildcardquery(tokens[i])))\n",
        "    else:\n",
        "      tokens1.append(tokens[i])\n",
        "  filtered_tfidf=[]\n",
        "  snippets=[]\n",
        "  for i in results:\n",
        "    for j in results[i]:\n",
        "      filtered_tfidf.append(record_list[i][1]+int(j))\n",
        "      snippets.append(preprocessed_snippet_df[filtered_tfidf[-1]])\n",
        "  results1=[]\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform(snippets)\n",
        "  query_vec = vectorizer.transform([preprocessing(\" \".join(tokens1))])\n",
        "  results1=cosine_similarity(X,query_vec).reshape((-1,))\n",
        "  results2=sorted(range(len(results1)),reverse=True, key=lambda x:results1[x])\n",
        "  t2=time.time()\n",
        "  print()\n",
        "  k=min(k,len(results2))\n",
        "  print(len(results2),\" results fetched in \",t2-t1,\"seconds. Top \",k,\"are being shown below - \")\n",
        "  print()\n",
        "  table=[]\n",
        "  for i in results2[:k]:\n",
        "      x=snippet_df[filtered_tfidf[i]]\n",
        "      table.append([x[1],x[2],x[0]])\n",
        "  print(tabulate(table,headers=[\"Document Name\",\"Row number\",\"Snippet\"]))\n",
        "  print()\n",
        "  for i in results2:\n",
        "      f.write(snippet_df[filtered_tfidf[i]][0])\n",
        "      f.write(\"\\n\")\n",
        "  f.close()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI4Rpe5IKcTx"
      },
      "source": [
        "# Spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ASgsvsTdWV7"
      },
      "source": [
        "def spellcheck(query):\n",
        "  spell = Speller(lang='en')\n",
        "  query1=query\n",
        "  query = query.split()\n",
        "  corrected=[]\n",
        "  for i in range(len(query)):\n",
        "    if query[i] in inverted_index:\n",
        "      corrected.append(query[i])\n",
        "    else:\n",
        "      corrected.append(spell(query[i]))\n",
        "  corrected=\" \".join(corrected)\n",
        "  if corrected!=query1:\n",
        "    print(\"Did you mean\",corrected,\"?\")\n",
        "  return corrected"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RdVeANx4kyn"
      },
      "source": [
        "# Mixed Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjh67xNrd4KB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a918922-61d7-466b-e674-5982758abc64"
      },
      "source": [
        "ranked_results('hello',10)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "280  results fetched in  0.10031819343566895 seconds. Top  10 are being shown below - \n",
            "\n",
            "Document Name         Row number  Snippet\n",
            "------------------  ------------  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "BBCNEWS.201806.csv            59  to tackle climate change and air pollution hello, everyone - this is afternoon live.\n",
            "MSNBC.201811.csv             182  you, too. and ahead, what those wildfires in california are telling us. behind the lost lives and burnt forest is the growing threat affect being us all, climate change. hello!. hello is friendly.\n",
            "BBCNEWS.201808.csv            77  climate change, making parts of the planet uninhabitable. hello, everyone, this is afternoon live.\n",
            "MSNBC.201612.csv             208  hello? hello? they lost the connection. we're back. that was actor bradley whitford in season two of the documentary about climate change 'years of living dangerously' airs december 7th. the president-elect has called\n",
            "BBCNEWS.201908.csv           620  about climate change. hello there. the last two weeks have been pretty unsettled across the uk,\n",
            "BBCNEWS.201907.csv           109  questionnaire, to the whole hustings. hello. when you are prime minister, how will you stop climate\n",
            "BBCNEWS.201705.csv           315  to set a cap on standard variable tariffs for gas and electricity if they win the election. the party says it would reduce bills for millions of households. joining me now is callum mccaig, the snp energy and climate change spokesperson. hello. good evening. hello there,\n",
            "MSNBC.201002.csv             105  fenn. hello, guys. hello alex. the snow and the aftermath, all that chilly between so many of the parties. pat could the snowstorm affect climate change legislation? i think climate change legislation in the form of cap and trade is completely dead.\n",
            "BBCNEWS.201910.csv           517  hello, i'm asad ahmad. two weeks of demonstrations by climate change activists hello, and welcome to sportsday - i'm gavin ramjaun. crunch time at the rugby world cup, with england up first tomorrow morning in their quarter final against australia. andy murray continues his impressive comeback to singles action -  _\n",
            "BBCNEWS.201809.csv            79  climate change rises with the temperature. hello and welcome to asia business report. as we have been reporting on newsday, japanese\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE6uiMoMLUug"
      },
      "source": [],
      "execution_count": 65,
      "outputs": []
    }
  ]
}